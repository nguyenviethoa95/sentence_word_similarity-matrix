{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing the following paper https://ieeexplore.ieee.org/document/1644735"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Nguyen Viet\n",
      "[nltk_data]     Hoa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet as wn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_in_wordnet(word):\n",
    "    wn_lemmas = set(wordnet.all_lemma_names())\n",
    "\n",
    "    if word in wn_lemmas:\n",
    "        return True\n",
    "    else: \n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_word_similarity(word1,word2):\n",
    "    first_word = wn.synsets(word1)[0]\n",
    "    second_word = wn.synsets(word2)[0]\n",
    "    return (first_word.wup_similarity(second_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_length(sen1, sen2):\n",
    "    len1= len(nltk.word_tokenize(sen1))\n",
    "    len2= len(nltk.word_tokenize(sen2))\n",
    "    return max(len1,len2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_sen = \"Three dogs pulling a man on a bicycle through the snow\"\n",
    "second_sen = \"The dogs are pulling a man on a type of bike through the snow\"\n",
    "maxlength = max_length(first_sen,second_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sen1 = [token in nltk.word_tokenize(first_sen)\n",
    "tokenized_sen2 = nltk.word_tokenize(first_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-f06eb9a14c5e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mcompute_word_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m             \u001b[0msum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompute_word_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-9003144881d5>\u001b[0m in \u001b[0;36mcompute_word_similarity\u001b[1;34m(word1, word2)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcompute_word_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mword2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mfirst_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0msecond_word\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfirst_word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwup_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msecond_word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "sentence = [tokenized_sen1,tokenized_sen2]\n",
    "count = len(tokenized_sen1)*len(tokenized_sen2)\n",
    "\n",
    "sum = 0.0\n",
    "for i in sentence[0]:\n",
    "    for j in sentence[1]:\n",
    "        if compute_word_similarity(i,j) != None:\n",
    "            sum = sum + float(compute_word_similarity(i,j))\n",
    "            \n",
    "print(sum/count)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def unique_string(str_):\n",
    "    words = str_.split(' ')\n",
    "    c = Counter(words)\n",
    "    return [w for w in words if c[w] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token1 = unique_string(first_sen)\n",
    "token2 = unique_string(second_sen)\n",
    "joint_token = token1 + token2\n",
    "print(joint_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0\n",
    "\n",
    "word_score1 = [] \n",
    "\n",
    "for token in joint_token: \n",
    "    if token in token1: \n",
    "        word_score1.append(1)\n",
    "    else:\n",
    "        max = 0\n",
    "        for word in token1: \n",
    "            if compute_word_similarity(token,word) != None:\n",
    "                score = compute_word_similarity(token,word)\n",
    "                if score > max : \n",
    "                    max = score\n",
    "        word_score1.append(max)\n",
    "    \n",
    "word_score2 = [] \n",
    "\n",
    "for token in joint_token: \n",
    "    if token in token2: \n",
    "        word_score2.append(1)\n",
    "    else:\n",
    "        max = 0\n",
    "        for word in token2: \n",
    "            if compute_word_similarity(token,word) != None:\n",
    "                score = compute_word_similarity(token,word)\n",
    "                if score > max : \n",
    "                    max = score\n",
    "        word_score2.append(max)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "semantic_score = dot(word_score1, word_score2)/(norm(word_score1)*norm(word_score2))\n",
    "print(semantic_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Order Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_the_most_similar_word(token, sen):\n",
    "    max = 0\n",
    "    index = 0\n",
    "    for i, word in enumerate(sen): \n",
    "        print(i,word)\n",
    "        if compute_word_similarity(token,word) != None:\n",
    "            score = compute_word_similarity(token,word)\n",
    "            if score > max : \n",
    "                max = score\n",
    "                index = i\n",
    "    return (index,max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(find_the_most_similar_word(\"dog\",token1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.7\n",
    "word_order_vec1 = []\n",
    "\n",
    "for token in joint_token: \n",
    "    if token in token1: \n",
    "        for index,word in enumerate(token1): \n",
    "            if word == token: \n",
    "                word_order_vec1.append(index)\n",
    "    else: \n",
    "        (index,score) = find_the_most_similar_word(token,token1)\n",
    "        if score >= threshold: \n",
    "            word_order_vec1.append(index)\n",
    "        else: \n",
    "            word_order_vec1.append(0)\n",
    "            \n",
    "word_order_vec2 = []\n",
    "for token in joint_token: \n",
    "    if token in token2: \n",
    "        for index,word in enumerate(token2): \n",
    "            if word == token: \n",
    "                word_order_vec2.append(index)\n",
    "    else: \n",
    "        (index,score) = find_the_most_similar_word(token,token2)\n",
    "        if score >= threshold: \n",
    "            word_order_vec2.append(index)\n",
    "        else: \n",
    "            word_order_vec2.append(0)\n",
    "\n",
    "print(word_order_vec2)\n",
    "print(word_order_vec1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "word_order_vec1 = np.array(word_order_vec1)\n",
    "word_order_vec2 = np.array(word_order_vec2)\n",
    "\n",
    "word_order_similarity = 1 - (norm(word_order_vec1-word_order_vec2)/norm(word_order_vec2+word_order_vec1))\n",
    "\n",
    "print(word_order_similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overall similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.7\n",
    "overall_similarity = alpha*semantic_score + (1-alpha)*word_order_similarity\n",
    "print(overall_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
